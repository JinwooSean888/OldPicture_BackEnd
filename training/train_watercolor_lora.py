# ğŸ“¦ Watercolor LoRA Training Pack
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ ìº”ë²„ìŠ¤ì—ëŠ” "ì–´ì œ ë§Œë“  ë°±ì—”ë“œ"ì™€ í˜¸í™˜ë˜ëŠ”, ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ì½”ë“œê°€
# íŒŒì¼ë³„ë¡œ í¬í•¨ë¼ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ íŒŒì¼ ë¸”ë¡ë“¤ì„ ê° ê²½ë¡œëŒ€ë¡œ ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.
#
# í¬í•¨ë¬¼
# 1) training/train_watercolor_lora.py    â† LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (SD1.5 ê¸°ë°˜)
# 2) training/prepare_dataset.py          â† ë°ì´í„° ì •ë¦¬/ì¦ê°•(ì„ íƒ)
# 3) training/sd_eval_img2img.py          â† í•™ìŠµ ê²°ê³¼ ë¹ ë¥¸ ìƒ˜í”Œ í™•ì¸(img2img)
# 4) training/requirements.txt            â† í•™ìŠµìš© ì˜ì¡´ì„±
# 5) training/README.md                   â† ì‚¬ìš©ë²• ìš”ì•½
#
# ì£¼ì˜
# - torch_dtype / dtype ê´€ë ¨ í˜¸í™˜ ì´ìŠˆ ë•Œë¬¸ì—, ì—¬ê¸°ì„œëŠ” "torch_dtype"ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# - CUDAê°€ ì•„ë‹ˆë©´ ìë™ìœ¼ë¡œ float32ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
# - Apple Silicon(M1/M2)ì€ MPSë¡œ ìë™ ì„ íƒë©ë‹ˆë‹¤.


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# File: training/train_watercolor_lora.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import os, math, random
from dataclasses import dataclass
from typing import Optional, List
from PIL import Image

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

from diffusers import StableDiffusionPipeline, DDPMScheduler
from diffusers.models.attention_processor import LoRAAttnProcessor
from diffusers.optimization import get_scheduler
from transformers import CLIPTokenizer

# ====== ì„¤ì • ======
MODEL_ID   = "runwayml/stable-diffusion-v1-5"  # ì‚¬ì „í•™ìŠµ ëª¨ë¸
DATA_DIR   = os.environ.get("DATA_DIR", "./dataset/images")
OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "./export/watercolor_lora")
TRIGGER    = os.environ.get("TRIGGER", "<wtr>")  # ìŠ¤íƒ€ì¼ í† í°

# ë””ë°”ì´ìŠ¤/ì •ë°€ë„ ìë™ ì„ íƒ
if torch.cuda.is_available():
    DEVICE, TORCH_DTYPE = "cuda", torch.float16
elif torch.backends.mps.is_available():
    DEVICE, TORCH_DTYPE = "mps", torch.float32
else:
    DEVICE, TORCH_DTYPE = "cpu", torch.float32

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ)
RES       = int(os.environ.get("RES", 512))
BATCH     = int(os.environ.get("BATCH", 2))
ACCUM     = int(os.environ.get("ACCUM", 4))              # ìœ íš¨ ë°°ì¹˜ = BATCH*ACCUM
EPOCHS    = int(os.environ.get("EPOCHS", 4))
LR_UNET   = float(os.environ.get("LR_UNET", 1e-4))
LR_TXT    = float(os.environ.get("LR_TXT", 5e-5))
SAVE_EVERY= int(os.environ.get("SAVE_EVERY", 400))
NUM_WORK  = int(os.environ.get("NUM_WORK", 2))
SEED      = int(os.environ.get("SEED", 42))

PROMPTS = [
    f"a {TRIGGER} style landscape, soft wash, delicate edges",
    f"portrait, {TRIGGER} style, watercolor texture, soft bleeding",
    f"city street, {TRIGGER} style illustration, hand-painted look",
    f"still life, {TRIGGER} style, subtle color wash",
]

random.seed(SEED)

torch.manual_seed(SEED)
if DEVICE == "cuda":
    torch.cuda.manual_seed_all(SEED)

@dataclass
class Item:
    path: str
    prompt: str

class ImgSet(Dataset):
    def __init__(self, root: str, res: int):
        assert os.path.isdir(root), f"Dataset dir not found: {root}"
        self.paths = [
            os.path.join(root, f)
            for f in os.listdir(root)
            if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))
        ]
        assert self.paths, f"No images found in {root}"
        self.t = transforms.Compose([
            transforms.Resize(res, transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(res),
            transforms.ToTensor(),                      # [0,1]
            transforms.Normalize([0.5],[0.5])           # -> [-1,1]
        ])

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, i):
        p = self.paths[i]
        img = Image.open(p).convert("RGB")
        img = self.t(img)
        return Item(p, random.choice(PROMPTS)), img


def inject_lora(unet, rank: int = 8):
    """
    diffusers ë²„ì „ì— ë”°ë¼ LoRA ì£¼ì… ì‹œê·¸ë‹ˆì²˜ê°€ ë‹¤ë¥¸ ë¬¸ì œë¥¼ í¡ìˆ˜.
    - ì‹ ë²„ì „: LoRAAttnProcessor(hidden_size=..., cross_attention_dim=..., rank=...)
    - êµ¬ë²„ì „: ì¸ì ì—†ëŠ” LoRAAttnProcessor()ë§Œ í—ˆìš© (rank ì¡°ì • ë¶ˆê°€)
    """
    attn_procs = {}
    new_api = version.parse(diffusers.__version__) >= version.parse("0.24.0")  # ëŒ€ëµì  ê²½ê³„

    for name in unet.attn_processors.keys():
        if "mid_block" in name:
            hidden_size = unet.config.block_out_channels[-1]
        elif "up_blocks" in name:
            i = int(name.split(".")[1])
            hidden_size = list(reversed(unet.config.block_out_channels))[i]
        elif "down_blocks" in name:
            i = int(name.split(".")[1])
            hidden_size = unet.config.block_out_channels[i]
        else:
            hidden_size = unet.config.block_out_channels[0]

        cross_dim = unet.config.cross_attention_dim

        if new_api:
            # ì‹ ë²„ì „ API
            attn_procs[name] = LoRAAttnProcessor(
                hidden_size=hidden_size,
                cross_attention_dim=cross_dim,
                rank=rank,
            )
        else:
            # êµ¬ë²„ì „ í˜¸í™˜: ì¸ì ì—†ëŠ” ìƒì„±ì ì‹œë„
            try:
                attn_procs[name] = LoRAAttnProcessor()
            except TypeError:
                # ì¼ë¶€ ì•„ì£¼ êµ¬ë²„ì „ ëŒ€ë¹„ ì™„ì¶©
                attn_procs[name] = LoRAAttnProcessor

    unet.set_attn_processor(attn_procs)


def save(pipe: StableDiffusionPipeline, out_dir: str):
    os.makedirs(out_dir, exist_ok=True)
    # UNetì˜ LoRA ê°€ì¤‘ì¹˜ ì €ì¥ (safetensors)
    pipe.unet.save_attn_procs(out_dir)
    # í† í¬ë‚˜ì´ì € ì €ì¥ (ìŠ¤íƒ€ì¼ í† í° í¬í•¨)
    pipe.tokenizer.save_pretrained(out_dir)
    # í…ìŠ¤íŠ¸ ì„ë² ë”© ì „ì²´ ë°±ì—…(ì„ íƒ)
    torch.save({
        "token": TRIGGER,
        "emb": pipe.text_encoder.get_input_embeddings().weight.detach().cpu()
    }, os.path.join(out_dir, "text_encoder_full.pt"))
    print(f"[save] {out_dir}")


def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    ds = ImgSet(DATA_DIR, RES)
    dl = DataLoader(ds, batch_size=BATCH, shuffle=True, num_workers=NUM_WORK, drop_last=True)

    # ì‚¬ì „í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipe = StableDiffusionPipeline.from_pretrained(
        MODEL_ID,
        torch_dtype=TORCH_DTYPE,   # â† êµ¬ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ torch_dtype ì‚¬ìš©
        safety_checker=None
    )
    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)
    pipe.to(DEVICE)

    tok: CLIPTokenizer = pipe.tokenizer

    # ìƒˆ ìŠ¤íƒ€ì¼ í† í° ì¶”ê°€ ë° ì´ˆê¸°í™”
    if TRIGGER not in tok.get_vocab():
        tok.add_tokens(TRIGGER)
        pipe.text_encoder.resize_token_embeddings(len(tok))
        with torch.no_grad():
            base_tok = "watercolor"
            base_id = tok.convert_tokens_to_ids(base_tok) if base_tok in tok.get_vocab() else tok.eos_token_id
            new_id  = tok.convert_tokens_to_ids(TRIGGER)
            base_emb = pipe.text_encoder.get_input_embeddings().weight[base_id].clone()
            pipe.text_encoder.get_input_embeddings().weight.data[new_id] = base_emb

    # LoRA ì£¼ì… (UNet)
    inject_lora(pipe.unet, rank=8)

    # ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬ (UNet LoRA + í…ìŠ¤íŠ¸ ì„ë² ë”©ë§Œ í•™ìŠµ)
    params = [
        {"params": [p for p in pipe.unet.parameters() if p.requires_grad], "lr": LR_UNET},
        {"params": [pipe.text_encoder.get_input_embeddings().weight], "lr": LR_TXT}
    ]
    opt = torch.optim.AdamW(params, weight_decay=1e-2)
    steps_total = EPOCHS * math.ceil(len(ds) / BATCH)
    sch = get_scheduler(
        name="cosine",
        optimizer=opt,
        num_warmup_steps=int(0.03 * steps_total),
        num_training_steps=steps_total
    )

    pipe.unet.train(); pipe.text_encoder.train()

    step = 0
    accum = 0
    opt.zero_grad(set_to_none=True)

    for epoch in range(EPOCHS):
        for (items, imgs) in dl:
            imgs = imgs.to(DEVICE, dtype=TORCH_DTYPE)

            # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
            prompts = [it.prompt for it in items]
            tokens = tok(
                prompts,
                padding="max_length",
                max_length=tok.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).to(DEVICE)
            enc = pipe.text_encoder(**tokens).last_hidden_state

            # ë…¸ì´ì¦ˆ ì£¼ì…(DDPM)
            with torch.no_grad():
                noise = torch.randn_like(imgs)
                timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (imgs.size(0),), device=imgs.device).long()
                noisy = pipe.scheduler.add_noise(imgs, noise, timesteps)

            # ì˜ˆì¸¡ ë° ì†ì‹¤
            pred = pipe.unet(noisy, timesteps, encoder_hidden_states=enc).sample
            loss = torch.nn.functional.mse_loss(pred.float(), noise.float()) / ACCUM
            loss.backward(); accum += 1

            if accum == ACCUM:
                torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), 1.0)
                opt.step(); sch.step(); opt.zero_grad(set_to_none=True); accum = 0

            if step % 50 == 0:
                print(f"[step {step}] loss={loss.item() * ACCUM:.4f}")
            if step and step % SAVE_EVERY == 0:
                save(pipe, os.path.join(OUTPUT_DIR, f"step{step}"))

            step += 1

    save(pipe, os.path.join(OUTPUT_DIR, "final"))


if __name__ == "__main__":
    # Mac(M1/M2) ì•ˆì •ì„±: ì¼ë¶€ op fallback í—ˆìš©
    if DEVICE == "mps":
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
    main()
